Primera parte:
En primer lugar, se cargan las librerías a utilizar. Luego se carga la base de datos y se visualizan las primeras filas. Por último se aprecia información acerca de los datos y un análisis descriptivo de éstos.

Segunda parte:
A continuación, se realiza análisis de componentes principales para reducir dimensionalidad, para lo cual se dejan de lado las variables "Name" y "GP", además de la variable a predecir "Target_5Yrs". Tras esto, la primera acción a realizar es la estandarización de los datos para luego obtener su matriz de correlación. Luego se realizan pruebas para determinar que sí se puede aplicar PCA. Seguido de esto, se obtiene la matriz de covarianza, los autovalores y los autovectores. A partir de lo último, so obtiene el porcentaje de varianza explicada de cada autovalor.
Se obtiene el número de componentes y la varianza explicada de cada uno (gráfico incluido). En este punto está definido que la cantidad de componentes equivale 4, se obtiene la matriz de correlación Pearson y al DataFrame de dichos componentes se le agrega la variable binaria a predecir ahora con el nombre "Target".  

Tercera parte:
Para esta etapa, se aplica el logaritmo Lazy Classifier, el cual permite visualizar varios modelos sin la necesidad de escribir códigos extensos. En otras palabras, es un aporte al brindar un uso más rápido y simple.
Lo primero que se debe llevar a cabo es definir la varibale dependiente y las variables independientes. Luego de esto, se deben segmentar los datos, instanciar el modelo y entrenar los modelos de regresión. Por último, se observan los modelos con cada una de sus métricas. Para un análisis más simple se genera un gráfico que, basado en la métrica "Balanced Accuracy", indica que los mejores modelos son Gaussian NB, Passive Aggressive Classifier, Quadratic Discriminant Analysis, Calibrated Classifier CV y Nearest Centroid, respectivamente.

Cuarta parte:
Se selecciona Random Forest como modelo supervisado de clasificación. Posteriormente se lleva a cabo la partición de datos y las funciones para generar y visualizar métricas. Se instancia el algoritmo, para luego generar las predicciones y la matriz de confusión. Esta última se puede visualizar gráficamente al igual que, posteriormente, la curva ROC. Para la última etapa, se ajustan los hiperparámetros, se obtiene el mejor modelo y las métricas finales. La exactitud del modelo, que corresponde al número de predicciones correctas divido en el número de predicciones totales, alcanza un 65%. No una cifra que pueda considerarse del todo buena, sin embargo, entendiendo el contexto de las predicciones del proyecto, no presenta complicaciones. 